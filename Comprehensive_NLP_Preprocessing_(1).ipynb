{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraGul0111/SaraGul/blob/main/Comprehensive_NLP_Preprocessing_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Sara Gul***"
      ],
      "metadata": {
        "id": "-zQGHkVNjX2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comprehensive NLP Preprocessing**\n"
      ],
      "metadata": {
        "id": "xWkuopT8YOyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "\n",
        "Natural Language Processing (NLP) is a crucial field in artificial intelligence that focuses on the interaction between computers and human language. Preprocessing is a vital step in any NLP project, as it prepares raw text data for further analysis or model training. This notebook will guide you through a comprehensive set of preprocessing techniques using a paragraph as an example."
      ],
      "metadata": {
        "id": "4gNILr6ZYaUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup\n",
        "Import the necessary libraries:"
      ],
      "metadata": {
        "id": "YlXtkMv5YwDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy scikit-learn pyspellchecker\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikIl_YSFZixW",
        "outputId": "bdb491d6-1b23-4707-b967-17a8e61bf250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_EDzImaYGhC",
        "outputId": "6cd9208b-8c3f-4d86-8118-f7c9ed07a328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sample Paragraph"
      ],
      "metadata": {
        "id": "ukAaMJFFZzLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"\n",
        "In 2023, AI technolgy advanced rapidley! ChatGPT-4 amazd users with its\n",
        "capabilities, while Tesla's self-driving cars loged 1,000,000+ miles.\n",
        "The globel AI market reached $150 billion. Despite concerns, 78 percent of\n",
        "companies planed to increase AI investments by 2025.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original paragraph:\")\n",
        "print(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg1VGFVcZ2gH",
        "outputId": "d1f9e6e0-8cab-4ab9-bf55-36d2e7770934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original paragraph:\n",
            "\n",
            "In 2023, AI technolgy advanced rapidley! ChatGPT-4 amazd users with its\n",
            "capabilities, while Tesla's self-driving cars loged 1,000,000+ miles.\n",
            "The globel AI market reached $150 billion. Despite concerns, 78 percent of\n",
            "companies planed to increase AI investments by 2025.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Preprocessing Steps**"
      ],
      "metadata": {
        "id": "v2j0z25XaBPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Lowercase Conversion\n",
        "Converting all text to lowercase helps in standardizing the text:"
      ],
      "metadata": {
        "id": "9I0chgeIaFT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "lowercase_text = to_lowercase(paragraph)\n",
        "print(\"\\nLowercase text:\")\n",
        "print(lowercase_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2dBX-FcaMcF",
        "outputId": "310a8e4b-b678-4f23-f704-866afacc3f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lowercase text:\n",
            "\n",
            "in 2023, ai technolgy advanced rapidley! chatgpt-4 amazd users with its\n",
            "capabilities, while tesla's self-driving cars loged 1,000,000+ miles.\n",
            "the globel ai market reached $150 billion. despite concerns, 78 percent of\n",
            "companies planed to increase ai investments by 2025.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Remove Punctuation\n",
        "We'll use regex to remove punctuation:"
      ],
      "metadata": {
        "id": "H6x3MLnmaYDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "no_punct_text = remove_punctuation(lowercase_text)\n",
        "print(\"\\nText after removing punctuation:\")\n",
        "print(no_punct_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nz1GcBFacRu",
        "outputId": "67d800a8-73e0-49e3-cee2-9437d05eac65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text after removing punctuation:\n",
            "\n",
            "in 2023 ai technolgy advanced rapidley chatgpt4 amazd users with its\n",
            "capabilities while teslas selfdriving cars loged 1000000 miles\n",
            "the globel ai market reached 150 billion despite concerns 78 percent of\n",
            "companies planed to increase ai investments by 2025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Tokenization\n",
        "Tokenization breaks down the text into individual words:"
      ],
      "metadata": {
        "id": "plHDMdBNazAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "tokens = tokenize_text(no_punct_text)\n",
        "print(\"\\nTokenized text:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eT8cBVba1vA",
        "outputId": "66ef97dc-fb97-4053-d2ca-4cc21e81f265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized text:\n",
            "['in', '2023', 'ai', 'technolgy', 'advanced', 'rapidley', 'chatgpt4', 'amazd', 'users', 'with', 'its', 'capabilities', 'while', 'teslas', 'selfdriving', 'cars', 'loged', '1000000', 'miles', 'the', 'globel', 'ai', 'market', 'reached', '150', 'billion', 'despite', 'concerns', '78', 'percent', 'of', 'companies', 'planed', 'to', 'increase', 'ai', 'investments', 'by', '2025']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Correct Misspellings\n",
        "We'll use the pyspellchecker library to correct misspellings:"
      ],
      "metadata": {
        "id": "6-BrEiMnbBY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spellings(tokens):\n",
        "    spell = SpellChecker()\n",
        "    return [spell.correction(word) if spell.correction(word) is not None else word for word in tokens]\n",
        "\n",
        "corrected_tokens = correct_spellings(tokens)\n",
        "print(\"\\nTokens after spell correction:\")\n",
        "print(corrected_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx26eGljbEyR",
        "outputId": "a023ba69-054c-4b3e-c5ec-15a65165754b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokens after spell correction:\n",
            "['in', '2023', 'ai', 'technology', 'advanced', 'rapidly', 'chatgpt4', 'amazed', 'users', 'with', 'its', 'capabilities', 'while', 'teslas', 'selfdriving', 'cars', 'loved', '1000000', 'miles', 'the', 'global', 'ai', 'market', 'reached', '150', 'billion', 'despite', 'concerns', '78', 'percent', 'of', 'companies', 'planed', 'to', 'increase', 'ai', 'investments', 'by', '2025']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Remove Stopwords\n",
        "Stopwords are common words that often don't contribute much to the meaning:"
      ],
      "metadata": {
        "id": "TuxmHAk1bcNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "filtered_tokens = remove_stopwords(corrected_tokens)\n",
        "print(\"\\nTokens after removing stopwords:\")\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slXBDkZ9bflu",
        "outputId": "7d1ad489-40a6-499b-fe40-3d6cefe78b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokens after removing stopwords:\n",
            "['2023', 'ai', 'technology', 'advanced', 'rapidly', 'chatgpt4', 'amazed', 'users', 'capabilities', 'teslas', 'selfdriving', 'cars', 'loved', '1000000', 'miles', 'global', 'ai', 'market', 'reached', '150', 'billion', 'despite', 'concerns', '78', 'percent', 'companies', 'planed', 'increase', 'ai', 'investments', '2025']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Stemming\n",
        "Stemming reduces words to their root form:"
      ],
      "metadata": {
        "id": "mnp6B93zbwih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_words(tokens):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "stemmed_tokens = stem_words(filtered_tokens)\n",
        "print(\"\\nStemmed tokens:\")\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5WbhaDAbzx9",
        "outputId": "911b3f90-910f-4dcd-8eab-19cb009b24c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemmed tokens:\n",
            "['2023', 'ai', 'technolog', 'advanc', 'rapidli', 'chatgpt4', 'amaz', 'user', 'capabl', 'tesla', 'selfdriv', 'car', 'love', '1000000', 'mile', 'global', 'ai', 'market', 'reach', '150', 'billion', 'despit', 'concern', '78', 'percent', 'compani', 'plane', 'increas', 'ai', 'invest', '2025']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Lemmatization\n",
        "Lemmatization is similar to stemming but produces more meaningful root forms:"
      ],
      "metadata": {
        "id": "v6jYGruVcKAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_words(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "lemmatized_tokens = lemmatize_words(filtered_tokens)\n",
        "print(\"\\nLemmatized tokens:\")\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XVDlzVScNM9",
        "outputId": "741b8b02-aa74-4662-cfa5-3166e0eb9a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized tokens:\n",
            "['2023', 'ai', 'technology', 'advanced', 'rapidly', 'chatgpt4', 'amazed', 'user', 'capability', 'tesla', 'selfdriving', 'car', 'loved', '1000000', 'mile', 'global', 'ai', 'market', 'reached', '150', 'billion', 'despite', 'concern', '78', 'percent', 'company', 'planed', 'increase', 'ai', 'investment', '2025']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Parts of Speech Tagging\n",
        "POS tagging assigns grammatical categories to each word:"
      ],
      "metadata": {
        "id": "DuINlkZZcue7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tagging(tokens):\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "pos_tagged = pos_tagging(lemmatized_tokens)\n",
        "print(\"\\nPOS tagged tokens:\")\n",
        "print(pos_tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcdjnoIccwtE",
        "outputId": "5825cf31-4602-4cfe-f47f-820e87617c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS tagged tokens:\n",
            "[('2023', 'CD'), ('ai', 'NN'), ('technology', 'NN'), ('advanced', 'VBD'), ('rapidly', 'RB'), ('chatgpt4', 'JJ'), ('amazed', 'VBN'), ('user', 'NN'), ('capability', 'NN'), ('tesla', 'VBP'), ('selfdriving', 'VBG'), ('car', 'NN'), ('loved', 'VBD'), ('1000000', 'CD'), ('mile', 'NN'), ('global', 'JJ'), ('ai', 'NN'), ('market', 'NN'), ('reached', 'VBD'), ('150', 'CD'), ('billion', 'CD'), ('despite', 'IN'), ('concern', 'NN'), ('78', 'CD'), ('percent', 'NN'), ('company', 'NN'), ('planed', 'VBD'), ('increase', 'NN'), ('ai', 'NN'), ('investment', 'NN'), ('2025', 'CD')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Named Entity Recognition\n",
        "NER identifies and classifies named entities in the text:"
      ],
      "metadata": {
        "id": "aK200ExYdAL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "ner_results = named_entity_recognition(\" \".join(lemmatized_tokens))\n",
        "print(\"\\nNamed Entities:\")\n",
        "print(ner_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frSJ4ApNdDRm",
        "outputId": "6419771f-2351-4651-c2a2-f27e408ca738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Named Entities:\n",
            "[('2023', 'DATE'), ('chatgpt4', 'ORG'), ('1000000 mile', 'QUANTITY'), ('150 billion', 'CARDINAL'), ('78 percent', 'PERCENT'), ('2025', 'DATE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Compound Term Extraction\n",
        "We'll use spaCy to extract compound terms (noun chunks):"
      ],
      "metadata": {
        "id": "Vvq5y4PJdP6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_compound_terms(text):\n",
        "    doc = nlp(text)\n",
        "    return [chunk.text for chunk in doc.noun_chunks]\n",
        "\n",
        "compound_terms = extract_compound_terms(\" \".join(lemmatized_tokens))\n",
        "print(\"\\nCompound Terms:\")\n",
        "print(compound_terms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DNmR9pYdWBU",
        "outputId": "94fc8bf0-6920-40ff-8e60-3c0412d95623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Compound Terms:\n",
            "['2023 ai technology', 'rapidly chatgpt4 amazed user capability tesla selfdriving car', '1000000 mile global ai market', 'concern 78 percent company', 'increase', 'investment']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Term Frequency (TF) Calculation\n",
        "Term Frequency measures how frequently a term occurs in a document."
      ],
      "metadata": {
        "id": "IGNF6Z00eqKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def calculate_tf(text):\n",
        "    tf_dict = Counter(text)\n",
        "    for word in tf_dict:\n",
        "        tf_dict[word] = tf_dict[word] / len(text)\n",
        "    return tf_dict\n",
        "\n",
        "# Calculate TF for our preprocessed text\n",
        "tf_scores = calculate_tf(lemmatized_tokens)\n",
        "print(\"\\nTerm Frequency (TF) Scores:\")\n",
        "for term, score in tf_scores.items():\n",
        "    print(f\"{term}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xn99ebUgV_d",
        "outputId": "dc818e8d-3e51-41c2-af4b-f789d53a1b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Term Frequency (TF) Scores:\n",
            "2023: 0.0323\n",
            "ai: 0.0968\n",
            "technology: 0.0323\n",
            "advanced: 0.0323\n",
            "rapidly: 0.0323\n",
            "chatgpt4: 0.0323\n",
            "amazed: 0.0323\n",
            "user: 0.0323\n",
            "capability: 0.0323\n",
            "tesla: 0.0323\n",
            "selfdriving: 0.0323\n",
            "car: 0.0323\n",
            "loved: 0.0323\n",
            "1000000: 0.0323\n",
            "mile: 0.0323\n",
            "global: 0.0323\n",
            "market: 0.0323\n",
            "reached: 0.0323\n",
            "150: 0.0323\n",
            "billion: 0.0323\n",
            "despite: 0.0323\n",
            "concern: 0.0323\n",
            "78: 0.0323\n",
            "percent: 0.0323\n",
            "company: 0.0323\n",
            "planed: 0.0323\n",
            "increase: 0.0323\n",
            "investment: 0.0323\n",
            "2025: 0.0323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12. Inverse Document Frequency (IDF) Calculation\n",
        "For a single document, IDF is calculated as the logarithm of the total number of terms divided by the number of times a specific term appears."
      ],
      "metadata": {
        "id": "X1SMsl6VeyM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_idf(text):\n",
        "    total_terms = len(text)\n",
        "    term_count = Counter(text)\n",
        "    idf_dict = {}\n",
        "    for term, count in term_count.items():\n",
        "        idf_dict[term] = math.log(total_terms / count)\n",
        "    return idf_dict\n",
        "\n",
        "# Calculate IDF for our preprocessed text\n",
        "idf_scores = calculate_idf(lemmatized_tokens)\n",
        "print(\"\\nInverse Document Frequency (IDF) Scores:\")\n",
        "for term, score in idf_scores.items():\n",
        "    print(f\"{term}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxBgxTZke2BZ",
        "outputId": "50573e97-2716-4a25-da89-56fc113f9578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inverse Document Frequency (IDF) Scores:\n",
            "2023: 3.4340\n",
            "ai: 2.3354\n",
            "technology: 3.4340\n",
            "advanced: 3.4340\n",
            "rapidly: 3.4340\n",
            "chatgpt4: 3.4340\n",
            "amazed: 3.4340\n",
            "user: 3.4340\n",
            "capability: 3.4340\n",
            "tesla: 3.4340\n",
            "selfdriving: 3.4340\n",
            "car: 3.4340\n",
            "loved: 3.4340\n",
            "1000000: 3.4340\n",
            "mile: 3.4340\n",
            "global: 3.4340\n",
            "market: 3.4340\n",
            "reached: 3.4340\n",
            "150: 3.4340\n",
            "billion: 3.4340\n",
            "despite: 3.4340\n",
            "concern: 3.4340\n",
            "78: 3.4340\n",
            "percent: 3.4340\n",
            "company: 3.4340\n",
            "planed: 3.4340\n",
            "increase: 3.4340\n",
            "investment: 3.4340\n",
            "2025: 3.4340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. TF-IDF Calculation\n",
        "Now we'll combine TF and IDF to get the TF-IDF scores. It measures the importance of words in the document"
      ],
      "metadata": {
        "id": "PccID4Uue8KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tfidf(tf_scores, idf_scores):\n",
        "    tfidf_scores = {}\n",
        "    for word, tf_score in tf_scores.items():\n",
        "        tfidf_scores[word] = tf_score * idf_scores[word]\n",
        "    return tfidf_scores\n",
        "\n",
        "# Calculate TF-IDF\n",
        "tfidf_scores = calculate_tfidf(tf_scores, idf_scores)\n",
        "print(\"\\nTF-IDF Scores:\")\n",
        "for term, score in sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{term}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMDy4Xn3fAIj",
        "outputId": "246fb7c9-b099-4c3d-8566-279e33c085e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Scores:\n",
            "ai: 0.2260\n",
            "2023: 0.1108\n",
            "technology: 0.1108\n",
            "advanced: 0.1108\n",
            "rapidly: 0.1108\n",
            "chatgpt4: 0.1108\n",
            "amazed: 0.1108\n",
            "user: 0.1108\n",
            "capability: 0.1108\n",
            "tesla: 0.1108\n",
            "selfdriving: 0.1108\n",
            "car: 0.1108\n",
            "loved: 0.1108\n",
            "1000000: 0.1108\n",
            "mile: 0.1108\n",
            "global: 0.1108\n",
            "market: 0.1108\n",
            "reached: 0.1108\n",
            "150: 0.1108\n",
            "billion: 0.1108\n",
            "despite: 0.1108\n",
            "concern: 0.1108\n",
            "78: 0.1108\n",
            "percent: 0.1108\n",
            "company: 0.1108\n",
            "planed: 0.1108\n",
            "increase: 0.1108\n",
            "investment: 0.1108\n",
            "2025: 0.1108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusion**\n",
        "In this notebook, we've demonstrated a comprehensive set of NLP preprocessing techniques on a custom paragraph. These steps are crucial for cleaning, standardizing, and extracting meaningful information from text data before further analysis or model training. Each step serves a specific purpose:\n",
        "\n",
        "Lowercase conversion: Standardizes text\n",
        "Punctuation removal: Cleans the text\n",
        "Tokenization: Breaks text into individual units\n",
        "Spell correction: Fixes misspellings\n",
        "Stopword removal: Eliminates less informative words\n",
        "Stemming: Reduces words to their root form\n",
        "Lemmatization: Produces meaningful root forms\n",
        "POS tagging: Assigns grammatical categories\n",
        "Named Entity Recognition: Identifies and classifies named entities\n",
        "Compound Term Extraction: Identifies multi-word expressions\n",
        "Term Frequency (TF) Calculation: Computes how often a word appears in the document\n",
        "Inverse Document Frequency (IDF) Calculation: Measures the importance of a word within the document\n",
        "TF-IDF Calculation: Combines TF and IDF to compute the importance of words in the document\n",
        "\n",
        "1.   Lowercase conversion: Standardizes text\n",
        "2.   Punctuation removal: Cleans the text\n",
        "1.   Tokenization: Breaks text into individual units\n",
        "2.   Spell correction: Fixes misspellings\n",
        "1.   Stopword removal: Eliminates less informative words\n",
        "2.   Stemming: Reduces words to their root form\n",
        "\n",
        "1.   Lemmatization: Produces meaningful root forms\n",
        "2.   POS tagging: Assigns grammatical categories\n",
        "\n",
        "1.   Named Entity Recognition: Identifies and classifies named entities\n",
        "2.   Compound Term Extraction: Identifies multi-word expressions\n",
        "\n",
        "1.   Term Frequency (TF) Calculation: Computes how often a word appears in the document\n",
        "2.   Inverse Document Frequency (IDF) Calculation: Measures the importance of a word within the document\n",
        "\n",
        "1.   TF-IDF Calculation: Combines TF and IDF to compute the importance of words in the document\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "By applying these preprocessing steps, we've transformed our original paragraph into a format that's more suitable for various NLP tasks."
      ],
      "metadata": {
        "id": "XkJ-hCW6g-rO"
      }
    }
  ]
}